# @package _global_
model: SimCLR_fusion
sigma_labels: 2.0
sigma: 5
drop_rate: 0.05
# pretrained_model_path: #path/to/cpkt
pretrained_model_path: /neurospin/dico/jchavas/Runs/70_self-supervised_two-regions/Output/2024-05-14/13-29-46_193/logs/lightning_logs/version_0/checkpoints/epoch=249-step=148000.ckpt #UKB
load_encoder_only: True
freeze_encoders: False
fusioned_latent_space_size: 512  # if set to -1, then it will be the sum of the encoders' output sizes
converter_activation: relu
